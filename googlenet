import tensorflow as tf
import numpy as np
import os
from skimage import io,transform

class Google():
    def __init__(self):
        self.x = tf.placeholder(tf.float32,[None,100,100,5],name='x')
        self.y = tf.placeholder(tf.int32,[None,],name='y')

        x = tf.layers.conv2d(self.x,64,7,2,padding='same',activation='relu',name='conv1')
        x = tf.layers.max_pooling2d(x,3,2,padding='same')
        x = tf.nn.lrn(x)

        x = tf.layers.conv2d(x,64,1,1,name='conv2')
        x = tf.layers.conv2d(x,192,3,1,padding='same',name='conv3')
        x = tf.nn.lrn(x)
        x = tf.layers.max_pooling2d(x,3,2,padding='same')
        x = self.inception(x,64,96,128,16,32,32,'3a')
        x = self.inception(x,128,128,192,32,96,64,'3b')
        x = tf.layers.max_pooling2d(x,3,2,padding='same')
        x = self.inception(x,192,96,208,16,48,64,'4a')
        x = self.inception(x,160,112,224,24,64,64,'4b')
        x = self.inception(x,128,128,256,24,64,64,'4c')
        x = self.inception(x,112,144,288,32,64,64,'4d')
        x = self.inception(x,256,160,320,32,128,128,'4e')
        x = tf.layers.max_pooling2d(x,3,2,padding='same')
        x = self.inception(x,256,160,320,32,128,128,'5a')
        x = self.inception(x,384,192,384,48,128,128,'5b')
        x = tf.layers.average_pooling2d(x,7,1,padding='same')
        x = tf.layers.flatten(x)
        x = tf.layers.dense(x,1024,activation='relu',use_bias=True)
        x = tf.nn.dropout(x,0.4)
        x = tf.layers.dense(x,512,activation='relu',use_bias=True)
        x = tf.nn.dropout(x,0.4)
        x = tf.layers.dense(x,2)

        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=x,labels=self.y))
        self.train = tf.train.AdamOptimizer(0.0001).minimize(self.loss)
        self.acc = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.argmax(x,1),tf.int32),self.y),tf.float32))


    def inception(self,x,kernal1,kernal13,kernal3,kernal15,kernal5,max_pool_out,name):
        x1 = tf.layers.conv2d(x,kernal1,1,1,padding='same',activation='relu',name=name+'1')
        x2_1 = tf.layers.conv2d(x,kernal13,1,1,padding='same',activation='relu',name=name+'3_1')
        x2_2 = tf.layers.conv2d(x2_1,kernal3,3,1,padding='same',activation='relu',name=name+'3_3')
        x3_1 = tf.layers.conv2d(x,kernal15,1,1,padding='same',activation='relu',name=name+'5_1')
        x3_2 = tf.layers.conv2d(x3_1,kernal5,5,1,padding='same',activation='relu',name=name+'5_5')
        x4 = tf.layers.max_pooling2d(x,3,1,padding='same')
        x4 = tf.layers.conv2d(x4,max_pool_out,1,1,padding='same',activation='relu',name=name+'pool_1')
        return tf.concat([x1,x2_2,x3_2,x4],3)


class Train():
    def __init__(self):
        self.tensor = Google()
        self.data = Data()
        self.saver = tf.train.Saver()
        self.sess = tf.Session()
        try:
            self.saver.restore(self.sess,MODEL_PATH)
            print('using a existing model')
        except:
            self.sess.run(tf.global_variables_initializer())
            print('using a new model')

    def train(self):
        batch_size = 16
        for i in range(100):
            for x,y in self.data.minibatch(self.data.data,self.data.label,batch_size):
                loss,_,acc = self.sess.run([self.tensor.loss,self.tensor.train,self.tensor.acc],feed_dict={self.tensor.x:x,self.tensor.y:y})
            if i%10 == 0:
                print('train_acc:',acc)
                print('loss:',loss)
        self.saver.save(self.sess,MODEL_PATH)

class Data():
    def __init__(self):
        floder = [os.path.join(PATH,x) for x in os.listdir(PATH) if os.path.isdir(os.path.join(PATH,x))]
        data,label = [],[]
        for i,j in enumerate(floder):
            img_list = [os.path.join(j,y) for y in os.listdir(j) if y.endswith('.tif')]
            for im in img_list:
                img = io.imread(im)
                img = np.transpose(img,(1,2,0))
                img = transform.resize(img,(100,100,5))
                print('reading %s'%im)
                data.append(img)
                label.append(i)

        le = len(data)
        lis = np.arange(le)
        np.random.shuffle(lis)
        self.data = np.asarray(data,np.float32)[lis]
        self.label = np.asarray(label,np.int32)[lis]

    def minibatch(self,x,y,batch_size):
        start = 0
        end = min(start+batch_size,x.shape[0])
        yield x[start:end],y[start:end]
        start+=batch_size

if __name__ == '__main__':
    PATH = r'F:\feng'
    MODEL_PATH = 'e:/google'
    google = Train()
    google.train()



